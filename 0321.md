# 🌈 Colab 사용법 익히기

## 💡 딥러닝 개발 프로세스
1. **Loading the Dataset (~.csv)**

📌 mount 코드:
```python
from google.colab import drive
drive.mount('/content/drive')
```

2. **Data preprocessing (Feature Engineering)**

- 데이터셋의 feature들을 분석
- 영향을 미칠 수 있는 새로운 feature 정의
- feature와 label 분리 후 표준화(Standardization) 수행

3. **Building Model (모델 구축)**

- ANN 구축: input layer, hidden layer, output layer 구성

4. **Training Model (모델 학습)**

- 하이퍼파라미터 튜닝
- 모델 학습 수행 (정확도 향상 목적)

5. **Survival Prediction (예측)**

- 예측 수행 및 submit

---

# 🧠 논문 Review

## 📘 논문 정보
- **제목**: "Can Large Language Model be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation"
- **주소**: [논문 링크](https://arxiv.org/abs/2402.13211)

## 🔍 연구 배경
- AI가 감정적인 대화를 잘 할 수 있나?
- Emotional Support Conversation(ESC)은 단순한 대화가 아닌 위로와 도움을 주는 대화

ESC의 3단계:
1. 탐색 (무슨 일이야?)
2. 위로 (그렇게 느끼는 게 당연해)
3. 행동 유도 (이렇게 하면 도움이 될 거야)

⚠️ BUT! AI가 위로 전략에만 편향될 위험

## ❓ 연구 질문
- AI는 왜 감정 지원을 잘하지 못할까?
  - 특정 전략에 과도 의존하는 경향
  - GPT는 위로만 반복, LLaMA는 질문만 반복

## 🛠️ 연구 방법
- 다양한 전략 사용 빈도 분석
- 편향을 줄이는 두 가지 방법 제시:
  1. Self-Contact: 다양한 예시 제공하여 AI 스스로 개선 유도
  2. External-Contact: 전략 가이드를 AI에 제공

📊 **결과**:
- Self-Contact → 효과 미미 or 오히려 더 편향됨
- External-Contact → 편향 감소, 대화 질 향상

## 🧾 연구 결과
- 전략 다양성을 확보할수록 유저는 AI 대화를 더 유용하게 평가

## 🎯 연구의 기여
- 감정 지원 대화의 편향 문제 제시 및 해결책 제안
- Preference bias의 실험적 입증
- 심리 상담, AI 감정 코칭 등에 활용 가능성

## 💬 내 생각
- AI는 단순 과제 도우미를 넘어 감정적 지원자로의 가능성 있음
- 특히 정신 건강 분야에서 큰 역할 기대
- AI가 인간 상담사의 원칙을 따르지 않아도 되기 때문에 더 자유롭고 안전한 대화 가능

---

# 📚 빅분기 준비: 빅데이터 모델링

### ✅ 분석 모형 정의 시 고려 사항
- **과대적합(Overfitting)**: 너무 학습해서 복잡
- **과소적합(Underfitting)**: 설명력이 낮음

📌 데이터셋 분리: 학습 / 검증 / 평가

---

### 📈 회귀 분석

- 다중 회귀: `y = w1x1 + w2x2 + b`
- 잔차: 예측값 - 실제값
- 최소제곱법 (Least Squares Method)

**통계 지표**
- SST: Total Sum of Squares
- SSE(RSS): Residual Sum of Squares
- SSR(ESS): Explained Sum of Squares
- R²: 결정계수 (설명력)

---

### ⚠️ 다중공선성 (Multicollinearity)
- VIF(분산팽창인수) > 10 → 다중공선성 존재
- `VIF = 1 / (1 - R²)`

---

### 🤖 회귀 모델 유형

- **릿지 회귀 / 라쏘 회귀**: 규제를 포함
- **교호항 포함 회귀**: 변수 간 상호작용 포함

---

### ⚙️ 로지스틱 회귀

- 범주형 데이터 분류
- Logit 변환 → 시그모이드 함수 → 확률값 도출
- x가 n 증가 → 확률이 eⁿ 증가

---

### 🌳 의사결정나무

- 분할 지표:
  - 분류: 지니지수, 엔트로피, 카이제곱
  - 회귀: 분산감소량

- 과적합 방지: 정지 규칙, 가지치기

---

### 🧠 인공신경망 (ANN)

- 선형 조합 + 활성 함수(sigmoid, relu 등) 사용
- XOR 문제 해결 가능
- Softmax: 다중 클래스 분류 시 확률 총합 1

**학습**
- 순전파 → 역전파 → 경사하강법

**과적합 방지**
- L1/L2 규제, Dropout (무작위 비활성화)

---

### ⚔️ 서포트 벡터 머신 (SVM)

- 마진이 가장 큰 초평면 찾기
- 커널 트릭 사용해 고차원에서 선형 분리

---

### 🛒 연관 분석

- 장바구니 분석 (Apriori 알고리즘)
- 주요 지표: 지지도, 신뢰도, 향상도

---

# 🐍 CS 공부 - Python

### 🔄 컴파일러 vs 인터프리터
- 컴파일러: 전체 코드 번역 후 실행 → 빠름
- 인터프리터: 한 줄씩 실행 → 디버깅 용이

### 🧠 객체

- Python의 모든 것이 객체 → 기본 타입도 객체
- 추가 메모리 사용 (C와 비교)

### 🗑️ 가비지 컬렉션 (Garbage Collection)

- 참조 카운팅 기반 메모리 관리

---

### ⚠️ 함수의 기본값 주의!

```python
def add_item(item, items=[]):
    items.append(item)
    return items

print(add_item(1))
print(add_item(2))  # 예상과 다르게 [1, 2] 출력
```

### 🧪 참조 문제

```python
a = [1, 2, 3]
b = a
b.append(4)
print(a)  # a도 변경됨!
```

---

### 🧭 절차지향 vs 객체지향

- 절차지향: 함수 중심
- 객체지향: 클래스 중심 + 객체 조작

---

### 🧩 for-else 문법

```python
for n in names:
    if n == name:
        print("Found")
        break
else:
    print("Not found")  # break 없이 끝나면 실행
```

---

### 📑 딕셔너리의 장점

- 리스트보다 탐색 속도 훨씬 빠름 (O(1) 접근)