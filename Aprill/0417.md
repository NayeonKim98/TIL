## 1. Mathematics for AI
`TIP` : 이 파트는 수학적 개념을 AI에서 왜 필요한지 연결지어 설명할 수 있는 능력

### 선형대수(Linear Algebra)
    - 행렬 곱(Matrix Multipication)
    - 고유값/고유벡터(Eigenvalue/Eigenvector)
    - SVD, 기저(Basis), 차원(Rank) 등
    - PCA, 연립방정식 해의 조건
    - Rank-Nullity Theorem, Span-Basis-Linear Independence, A = UㄷV^T

### 확률과 통계(Probability & Statistics)
    - 조건부 확률(Conditional Probability), 베이즈 정리(Baye's Theorem) 
    - 확률분포(Normal distribution, Bernoulli 등), 기댓값(Expected Value)
    - 기대값, 분산, 공분산, 상관계수, 베이즈 정리
    - 정규분포 + 중심극한정리 조합
    - 베이즈 정리 + 예시 적용형 문제제

### 미분/적분(Calculus)
    - 함수의 극한, 연속성, 미분 가능성
    - 최적화(Optimization), Gradient, Hessian 등


## 2. 자료구조 및 알고리즘
`TIP` : 구현 문제보다는 자료구조의 구조적 특성과 시간복잡도 설명

- 정렬 알고리즘(퀵, 머지, 힙)
- 탐색 알고리즘(DFS, BFS, 이진탐색)
- 그래프 알고리즘 (Dikstra, MST, Topological Sort)
- 시간복잡도 분석 (Big-O Notation)

- 기초 자료 구조 : 배열VS연결리스트, BST 삽입/삭제, DFS vs BFS
- 정렬 : 퀵, 머지, 힙 정렬 비교
- 해시 : 해시 충돌 해결 방식
- 기본 구현 : 피보나치, 중복 제거

- DFS/BFS 비교 + 코드 일부 채우기
- 정렬 알고리즘의 시간복잡도와 적합한 상황 제시시


## 3. 인공지능의 기초
`TIP` : 모델구조를 단순히 아는걸 넘어 왜 이 구조가 필요한가, 무엇이 문제를 해결하는가. 기초 모델의 수학 + 구조적 차이에 대해 쓰는 문제. 설명&한계

- 머신러닝의 기초 : 지도학습 VS 비지도학습
- 딥러닝 구조 : 퍼셉트론, 다층 퍼셉트론(MLP), 역전파(Backpropagation)
- 손실함수(Loss Function)와 최적화 알고리즘(Gradient Descent)
- 정규화(Regularization), Overfitting과 Dropout
- 성능 평가 지표 : Precision, Recall, F1 Score

### 실습형/데이터 분석 문제

- 전처리/결측치 : Null 처리 전략
- 모델 평가 : Precision/Recall/F1
- 교차 검증 : K-Fold의 원리
- 전처리 중요성 : 왜 normalization, encoding이 필요한가?
- Gradient Descent : 학습률과 수렴
- 정규화 기법 : Dropout, L2, EarlyStopping 차이점점
- 간단 코드 문제 : classification 구현, 중복 제거

- loss function (MSE, Cross Entropy)
- Activation Function (ReLU, Sigmoid, Tanh)
- Optimizer (Gradient Descent, Adam)

## Materials

- Introduction to Linear Algebra : Gilbert Strang
- AI를 위한 필수 수학 : 한빛미디어
- Probability and Statistics for Engineering and the Sciences

- Deep Learning : lan Goodfellow
- Pattern Recognition and Machine Learning by Bishop
- MIT AI Lecture
- Stanford CS231n
- CS50

- Deep Learning Book 1~5장 증명 연습
- LeetCode Medium 난이도 문제 30제 Python 수학 문제 구현 연습
- kaggle의 의료, 금융 데이터 프로젝트 실습사례 기반 논술 정리 훈련련

- 트렌디한 논문 요약 필


`Tip`

- 최근 3년간 기출문제 구하기
- 과목당 1시간 내외, 2~3시간 소요.
- 서술형 + 간단 계산

- 개념을 설명해라. 응용해라.비교하라.
EX_) "Cross-entropy와 MSE의 차이점은?",
" SVM과 로지스틱 회귀의 차이점은?"

- 단순 구현 X, 알고리즘의 핵심 개념과 시간 복잡도 설명.
EX_) "왜 이 문제에서는 Dijkstra를 쓰는게 맞는가?", 
" 이 구조를 일반 Queue가 아닌 Priority Queue로 구현해야하는 이유는?"


## 1. Mathematics for AI

### 선형대수(Linear Algebra)

1. 다음 연립방정식의 해가 존재하기 위한 조건을 설명하고, 해의 개수를 판별하시오.
1 2 3  x  a
2 4 6  y  b
3 6 9  z  c

2. 고유값과 고유벡터가 왜 PCA에 중요한지 서술하시오.

3. 특잇값 분해(SVD)의 정의와 응용 사례를 설명하시오.

4. 다음 행렬 A에 대해 대각화 가능 여부를 판단하시오.
4 1
2 3

5. 행렬 A에 대해 Ax = b의 해가 존재하는 조건을 설명하고, 해가 유일하지 않은 경우를 예시로 드시오.

6. PCA의 수학적 원리를 설명하고, SVD와의 관계를 서술하시오.

7. 고유값과 고유벡터를 정의하고, 이 개념이 어떤 머신러닝 모델에 사용되는지 예시를 드시오.

8. SVD가 어떤 문제에서 유용하게 사용되는지 설명하시오.

### 확률과 통계(Probability & Statistics)

1. 베이즈 정리를 이용해 스팸 메일 분류 문제를 설명하시오.

2. 정규분포의 평균과 분산을 정의하고, 왜 중앙값과 평균이 같은지 설명하시오.

3. 이항분포와 포아송 분포의 차이점을 설명하시오.

4. 샘플 평균과 표본 분산이 어떤 조건에서 불편추정량이 되는지 설명하시오.

5. 베이즈 정리를 수식으로 서술하고, 사전확률과 사후확률의 차이를 설명하시오.

6. 이항분포와 정규분포의 차이점을 설명하고, 중심극한정리(CLT)의 의미를 설명하시오.

7. 기댓값과 분산의 정의 및 성질을 서술하시오.

8. 두 확률 변수의 공분산과 상관계수의 차이를 설명하시오.


### 미분/적분(Calculus)

1. 다변수 함수 f(x, y)의 극값을 찾는 방법을 설명하고, Hessian matrix의 역할을 서술하시오.

2. 경사 하강법의 원리를 설명하고, local minima 문제에 대해 서술하시오.

3. 로지스틱 회귀에서의 loss function의 미분 과정을 직접 유도하시오.


## 2. 자료구조 및 알고리즘

1. 시간복잡도가 각각 O(n log n)인 정렬 알고리즘 2 가지를 설명하고, 서로의 차이를 비교하시오.

2. DFS와 BFS의 차이점과 각각의 사용 예시를 서술하시오.

3. 다익스트라(Dikstra) 알고리즘을 직접 설명하고, 음수 간선이 있을 경우 문제가 되는 이유를 서술하시오.

4. 힙 정렬의 동작 과정을 예를 들어 설명하시오.

5. 주어진 트리에서 가장 긴 경로를 구하는 알고리즘을 제시하시오.

6. 다이나믹 프로그래밍(DP)의 핵심 개념을 설명하고, 피보나치 수열 문제를 DP 방식으로 해결하시오.

7. 해시테이블의 충돌 처리 방식 중 open addressing과 chaining을 비교하시오.

8. 퀵정렬, 병합정렬, 힙정렬의 시간복잡도 및 동작 원리를 비교하시오.

9. 해시 테이블의 구조와 충돌 해결 방식을 설명하시오.

10. Python 또는 C++로 재귀적 피보나치 구현 코드를 작성하시오.

## 3. 인공지능의 기초

### 머신러닝 / 딥러닝 관련

1. 퍼셉트론의 구조를 설명하고, 학습 알고리즘의 수식 유도 과정을 서술하시오.

2. 로지스틱 회귀와 서포트 벡터 머신(SVM)의 차이점을 분류 경계의 관점에서 비교하시오.

3. 오버피팅과 언더피팅의 차이를 설명하고, 이를 방지하는 방법 3가지를 제시하시오.

4. 신경망에서 backpropagation의 원리를 간단한 예로 설명하시오.

5. Cross-entropy loss와 Meam Squared Error(MSE)의 차이와 사용 용도에 대해 서술하시오.

6. Gradient Descent와 Stochastic Gradient Descent(SGD)의 차이점을 설명하고, 왜 SGD가 대규모 데이터에서 유리한지 설명하시오.

7. 로지스틱 회귀와 SVM의 차이를 분류 경계의 형태, 수학적 관점에서 비교하시오.

8. 퍼셉트론의 구조와 학습 방식(가중치 갱신법)을 서술하시오.

9. 역전파의 개념과 사용되는 수학 원리를 설명하시오.

10. KNN 알고리즘의 원리와 장단점을 서술하시오.

11. CNN과 RNN의 구조적 차이를 설명하고, 각각의 장단점을 활용할 수 있는 실제 문제를 제시하시오.

12. Transformer의 구조를 설명하고, Self-Attention이 어떤 역할을 하는지 서술하시오.

13. 과적합을 방지하기 위한 기법 3 가지를 설명하시오.

14. Batch Normalization의 원리와 학습 과정에서의 이점을 서술하시오.

15. F1-score를 포함한 모델 평가 지표들을 설명하고, 불균형 데이터 상황에서 precision과 recall 중 어느 쪽이 중요한지 설명하시오.

16. 교차검증의 개념과 과적합 방지와의 관계를 설명하시오.

17. 결측지 처리 방법 3가지와 그 장단점을 서술하시오.

18. 하이퍼파라미터 튜닝의 필요성과 GridSearch, RandomSearch의 차이점을 서술하시오.

## 4. 종합형

1. SVM, Decision Tree, Logistic Regression 중 어떤 알고리즘을 어떤 상황에서 선택할지에 대한 비교 설명을 쓰시오.

2. PCA와 LDA의 차이를 설명하고, 각각의 수학적 원리를 서술하시오.

3. 딥러닝 모델에서 Dropout의 역할을 설명하고, Batch Normalization과 함께 사용할 때의 효과를 논의하시오.

4. 다층 퍼셉트론과 CNN의 차이를 입력 구조와 학습 구조 측면에서 설명하시오.


## 최종 TOP 10 예상 문제 유형

- PCA 수학 원리 + SVD와의 관계
- 베이즈 정리 수식 + 예시 적용
- 로지스틱 vs SVM
- CNN vs RNN 구조 및 차이
- Transformer 구조 + Self-Attention 설명
- DFS vs BFS 차이 및 활용 예시
- 피보나치 수열 재귀 구현
- F1-score 중심의 모델 성능 지표
- 과적합 방지 방법 (Dropout, EarlyStopping 등)
- AI의 윤리적 문제와 적용 분야에서의 쟁점


## 기출 

`2023 후기` : 조건부 확률 + 베이즈 - MLE 추정 - SVD 유도 - 트리 순회 - Transformer 설명
`2024 전기` : 이산수학(DFS) - GAN의 loss function 유도 - Pandas 결측치 처리 코드
`2025 예측` : - 의료/금융 시계열 데이터를 사용한 모델 적용 - 트랜스포머 구조와 한계 비교 - 딥러닝 Regularization 기법 비교

